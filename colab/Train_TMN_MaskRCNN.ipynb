{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b44d91",
   "metadata": {},
   "source": [
    "# Hızlı Başlangıç\n",
    "\n",
    "Bu defteri Colab'da güvenle çalıştırmak için özet adımlar:\n",
    "\n",
    "- GPU: Runtime → Change runtime type → GPU seçin.\n",
    "- Sıra: 1) Drive mount, 2) Kütüphaneler, 3) Config, 4) Dataset/DataLoader, 5) Model, 6) Train, 7) Eval, 8) (Opsiyonel) Görselleştirme.\n",
    "- Config (Cell 5):\n",
    "  - `OUT_ROOT`/`IMG_ROOT`: TMN çıktıları (ds2_dense_tmn) klasörlerinize işaret etmeli.\n",
    "  - `BATCH_SIZE`: Küçük başlayın (2), A100’de stabil ise artırın.\n",
    "  - `EVAL_SPLIT`: 'train' veya 'test' (mAP raporu bu split’e göre yazılır).\n",
    "  - `MAX_INSTANCES`: `None` = sınırsız; RAM kısıtlıysa makul bir sayı verin.\n",
    "  - `DATA_LOADER_PIN_MEMORY`: Colab RAM baskısını azaltmak için varsayılan False.\n",
    "- Çıktılar: `RUN_DIR` otomatik oluşturulur (`maskrcnn/YYYYMMDD_HHMM`).\n",
    "  - `logs/` (eğitim günlüğü), `checkpoints/` (ağırlıklar), `reports/` (mAP ve tespitler) burada.\n",
    "- Değerlendirme: `EVAL_SPLIT`’e göre mAP yazılır: `reports/metrics_<split>.json`.\n",
    "- İpuçları: `num_workers=0`, `pin_memory=False` Colab stabilitesine yardımcı olur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b481e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab: Mount Google Drive\n",
    "import sys, os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print('Drive mounted at /content/drive')\n",
    "else:\n",
    "    print('Not in Colab; skipping drive mount.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d24e425",
   "metadata": {},
   "source": [
    "# TMN Mask R-CNN Training (Dedicated Notebook)\n",
    "This notebook focuses only on training to avoid mixing with augmentation tasks. It sets up GPU, loads DS2 Dense TMN from Drive, trains Mask R-CNN, and saves checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proje Yapılandırması ve Kütüphanelerin İçe Aktarılması\n",
    "import os, sys, json, glob, time\n",
    "import torch, torchvision\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "print('GPU:', torch.cuda.is_available(), torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c52ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bölümler\n",
    "\n",
    "Bu defter şu bölümlerden oluşur:\n",
    "- preprocess: veri yolları ve dataloader hazırlığı\n",
    "- train: model kurulumu ve eğitim\n",
    "- eval: görselleştirme ve mAP değerlendirme\n",
    "- logs: eğitim günlükleri ve checkpoint konumu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cc883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametreleştirme: Config Sözlüğü ve Dataclass\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Eğitim girdileri: TMN dataset\n",
    "    OUT_ROOT: str = '/content/drive/MyDrive/omr_dataset/dataset/ds2/ds2_dense_tmn'\n",
    "    IMG_ROOT: str = '/content/drive/MyDrive/omr_dataset/dataset/ds2/ds2_dense_tmn/images'\n",
    "    # Eğitim çıktıları: ayrı bir train klasöründe kalıcı\n",
    "    TRAIN_ROOT: str = '/content/drive/MyDrive/omr_dataset/dataset/ds2/train/ds2_dense_tmn'\n",
    "    # Model özel alt klasörü\n",
    "    MASKRCNN_ROOT: str = '/content/drive/MyDrive/omr_dataset/dataset/ds2/train/ds2_dense_tmn/maskrcnn'\n",
    "    # Bellek için daha konservatif başlangıç batch boyutu\n",
    "    BATCH_SIZE: int = 2\n",
    "    EPOCHS: int = 3\n",
    "    LR: float = 0.005\n",
    "    MOMENTUM: float = 0.9\n",
    "    WEIGHT_DECAY: float = 0.0005\n",
    "    # Bellek kontrolleri\n",
    "    MAX_INSTANCES: Optional[int] = None   # Görsel başına sınır yok (RAM yeterliyse)\n",
    "    DATA_LOADER_PIN_MEMORY: bool = False  # RAM tüketimini azaltmak için kapalı\n",
    "    # Değerlendirme ayarları\n",
    "    EVAL_SPLIT: str = 'test'   # 'test' yoksa 'train' olarak ayarla\n",
    "    EVAL_SCORE_THR: float = 0.05\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)\n",
    "\n",
    "# Çalışma klasörü: tarih/saat etiketli alt klasör (maskrcnn altında)\n",
    "import time, os\n",
    "os.makedirs(cfg.MASKRCNN_ROOT, exist_ok=True)\n",
    "RUN_DIR = os.path.join(cfg.MASKRCNN_ROOT, time.strftime('%Y%m%d_%H%M'))\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "print('RUN_DIR:', RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2011c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kategori Haritası: category_id -> eğitim etiketi (1..K), 0 arkaplan\n",
    "import json, glob, os\n",
    "\n",
    "train_jsons = sorted(glob.glob(f\"{cfg.OUT_ROOT}/jsonlar/*train*.json\"))\n",
    "test_jsons = sorted(glob.glob(f\"{cfg.OUT_ROOT}/jsonlar/*test*.json\"))\n",
    "\n",
    "def build_category_maps(json_paths):\n",
    "    cats = set()\n",
    "    for jp in json_paths:\n",
    "        try:\n",
    "            with open(jp, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        except Exception:\n",
    "            continue\n",
    "        anns = data.get('annotations') or {}\n",
    "        ann_iter = anns.values() if isinstance(anns, dict) else anns\n",
    "        for a in ann_iter:\n",
    "            cats_val = a.get('cat_id') or a.get('category_id')\n",
    "            if isinstance(cats_val, list):\n",
    "                for c in cats_val:\n",
    "                    try:\n",
    "                        cats.add(int(c))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            elif cats_val is not None:\n",
    "                try:\n",
    "                    cats.add(int(cats_val))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    cats = sorted(cats)\n",
    "    cat_map = {orig: i+1 for i, orig in enumerate(cats)}  # 0: background\n",
    "    inv_cat_map = {v: k for k, v in cat_map.items()}\n",
    "    return cat_map, inv_cat_map\n",
    "\n",
    "ALL_JSONS = train_jsons + test_jsons\n",
    "CAT_MAP, INV_CAT_MAP = build_category_maps(ALL_JSONS)\n",
    "NUM_CLASSES = 1 + len(CAT_MAP)\n",
    "print({'num_classes': NUM_CLASSES, 'categories': len(CAT_MAP)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b837c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kategori Özeti: ID ve (varsa) isimleri yazdır\n",
    "import json\n",
    "\n",
    "def _collect_category_names(json_paths):\n",
    "    names = {}\n",
    "    for jp in json_paths:\n",
    "        try:\n",
    "            with open(jp, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        except Exception:\n",
    "            continue\n",
    "        cats = data.get('categories') or []\n",
    "        if isinstance(cats, dict):\n",
    "            cats = list(cats.values())\n",
    "        for c in cats:\n",
    "            try:\n",
    "                cid = int(c.get('id'))\n",
    "            except Exception:\n",
    "                continue\n",
    "            nm = c.get('name') or c.get('category_name') or None\n",
    "            if cid not in names and nm:\n",
    "                names[cid] = str(nm)\n",
    "    return names\n",
    "\n",
    "NAME_BY_CAT_ID = _collect_category_names(ALL_JSONS)\n",
    "\n",
    "ORIG_CATEGORY_IDS = sorted(CAT_MAP.keys())\n",
    "TRAIN_LABEL_TO_CATEGORY = {tr: INV_CAT_MAP[tr] for tr in sorted(INV_CAT_MAP.keys())}\n",
    "\n",
    "print('Orijinal category_id listesi (sıralı):', ORIG_CATEGORY_IDS)\n",
    "print('Toplam kategori:', len(ORIG_CATEGORY_IDS))\n",
    "print('\\nEğitim label -> category_id (ve ad):')\n",
    "for tr in range(1, NUM_CLASSES):\n",
    "    cid = TRAIN_LABEL_TO_CATEGORY.get(tr)\n",
    "    nm = NAME_BY_CAT_ID.get(cid)\n",
    "    if nm is not None:\n",
    "        print(f'  {tr:2d} -> {cid}  ({nm})')\n",
    "    else:\n",
    "        print(f'  {tr:2d} -> {cid}')\n",
    "\n",
    "# Opsiyonel: dizi olarak sakla\n",
    "TRAIN_CATEGORY_IDS = [TRAIN_LABEL_TO_CATEGORY[i] for i in range(1, NUM_CLASSES)]\n",
    "TRAIN_CATEGORY_NAMES = [NAME_BY_CAT_ID.get(cid) for cid in TRAIN_CATEGORY_IDS]\n",
    "print('\\nTRAIN_CATEGORY_IDS:', TRAIN_CATEGORY_IDS)\n",
    "print('TRAIN_CATEGORY_NAMES:', TRAIN_CATEGORY_NAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04363aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Günlükleme (Logging)\n",
    "import logging, os\n",
    "LOG_DIR = os.path.join(RUN_DIR, 'logs')\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "logging.basicConfig(level=logging.INFO, handlers=[\n",
    "    logging.FileHandler(os.path.join(LOG_DIR, 'train.log')),\n",
    "    logging.StreamHandler()\n",
    "])\n",
    "logging.info('Logging initialized at %s', LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri İşleme Akışı ve Dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DS2TMNDataset(Dataset):\n",
    "    def __init__(self, images_dir, json_paths, transform=None, max_instances=None, category_map=None):\n",
    "        import json\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.max_instances = max_instances\n",
    "        self.category_map = category_map\n",
    "        self.items = []  # {'filename': str, 'image_id': int}\n",
    "        self.anns_by_fn = {}  # filename -> List[[x1,y1,x2,y2,label]]\n",
    "\n",
    "        seen_fn = set()\n",
    "        for jp in json_paths:\n",
    "            with open(jp, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            imgs = data.get('images') or []\n",
    "            if isinstance(imgs, dict):\n",
    "                imgs = list(imgs.values())\n",
    "            id_to_fn = {}\n",
    "            for im in imgs:\n",
    "                fn = im.get('filename') or im.get('file_name')\n",
    "                if not fn:\n",
    "                    continue\n",
    "                try:\n",
    "                    iid = int(im.get('id')) if im.get('id') is not None else None\n",
    "                except Exception:\n",
    "                    iid = None\n",
    "                if iid is not None:\n",
    "                    id_to_fn[iid] = fn\n",
    "\n",
    "            anns = data.get('annotations') or {}\n",
    "            ann_iter = anns.values() if isinstance(anns, dict) else anns\n",
    "            for a in ann_iter:\n",
    "                img_id = a.get('img_id') or a.get('image_id')\n",
    "                if img_id is None:\n",
    "                    continue\n",
    "                try:\n",
    "                    fn = id_to_fn[int(img_id)]\n",
    "                except Exception:\n",
    "                    continue\n",
    "                b = a.get('a_bbox') or a.get('bbox')\n",
    "                cats = a.get('cat_id') or a.get('category_id') or []\n",
    "                if not b or len(b) < 4:\n",
    "                    continue\n",
    "                # Etiket çözümleme (liste/tekil)\n",
    "                if isinstance(cats, list) and len(cats) > 0:\n",
    "                    orig_lab = int(cats[0])\n",
    "                elif isinstance(cats, (int, str)):\n",
    "                    orig_lab = int(cats)\n",
    "                else:\n",
    "                    orig_lab = 0\n",
    "                # Eğitim için map'lenmiş etiket (1..K), 0 arkaplan kullanılmaz\n",
    "                if self.category_map is not None:\n",
    "                    mapped = self.category_map.get(orig_lab)\n",
    "                    if mapped is None:\n",
    "                        # bilinmeyense atla\n",
    "                        continue\n",
    "                    lab_to_use = int(mapped)\n",
    "                else:\n",
    "                    lab_to_use = int(orig_lab)\n",
    "                self.anns_by_fn.setdefault(fn, []).append([\n",
    "                    float(b[0]), float(b[1]), float(b[2]), float(b[3]), lab_to_use\n",
    "])\n",
    "\n",
    "            # Bu shard'daki görselleri ekle (fn bazlı tekil)\n",
    "            for iid, fn in id_to_fn.items():\n",
    "                if fn in seen_fn:\n",
    "                    continue\n",
    "                seen_fn.add(fn)\n",
    "                # image_id'i bu shard'dan alıyoruz; test/train ayrıldığı için mAP eşleşmesi korunur\n",
    "                self.items.append({'filename': fn, 'image_id': int(iid) if iid is not None else -1})\n",
    "\n",
    "        # Görselleri alfabetik sırala (tekrar üretilebilirlik için)\n",
    "        self.items.sort(key=lambda x: x['filename'])\n",
    "        self.to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im = self.items[idx]\n",
    "        fn = im['filename']\n",
    "        img_id = int(im['image_id'])\n",
    "        path = os.path.join(self.images_dir, fn)\n",
    "        try:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "        except Exception:\n",
    "            W = H = 1\n",
    "            img = Image.new('RGB', (W, H), (0, 0, 0))\n",
    "            lst = []\n",
    "        else:\n",
    "            W, H = img.size\n",
    "            lst = self.anns_by_fn.get(fn, [])\n",
    "\n",
    "        boxes_labels = []\n",
    "        for rec in lst:\n",
    "            x1, y1, x2, y2, lab = rec\n",
    "            x1 = max(0, min(x1, W - 1)); y1 = max(0, min(y1, H - 1))\n",
    "            x2 = max(0, min(x2, W));     y2 = max(0, min(y2, H))\n",
    "            if x2 > x1 and y2 > y1:\n",
    "                boxes_labels.append((x1, y1, x2, y2, lab))\n",
    "\n",
    "        # Instance sınırı (bellek için)\n",
    "        if self.max_instances and len(boxes_labels) > self.max_instances:\n",
    "            boxes_labels = boxes_labels[: self.max_instances]\n",
    "\n",
    "        boxes = [[x1, y1, x2, y2] for x1, y1, x2, y2, _ in boxes_labels]\n",
    "        labels = [lab for *_, lab in boxes_labels]\n",
    "\n",
    "        if len(boxes) > 0:\n",
    "            masks = torch.zeros((len(boxes), H, W), dtype=torch.bool)\n",
    "            for i, (x1, y1, x2, y2) in enumerate(boxes):\n",
    "                masks[i, int(y1):int(y2), int(x1):int(x2)] = True\n",
    "        else:\n",
    "            masks = torch.zeros((0, H, W), dtype=torch.bool)\n",
    "\n",
    "        target = {\n",
    "            'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels, dtype=torch.int64),\n",
    "            'masks': masks,\n",
    "            'image_id': torch.tensor([img_id], dtype=torch.int64)\n",
    "        }\n",
    "        img = self.transform(img) if self.transform else self.to_tensor(img)\n",
    "        return img, target\n",
    "\n",
    "train_jsons = sorted(glob.glob(f\"{cfg.OUT_ROOT}/jsonlar/*train*.json\"))\n",
    "test_jsons = sorted(glob.glob(f\"{cfg.OUT_ROOT}/jsonlar/*test*.json\"))\n",
    "train_ds = DS2TMNDataset(images_dir=cfg.IMG_ROOT, json_paths=train_jsons, max_instances=cfg.MAX_INSTANCES, category_map=CAT_MAP)\n",
    "test_ds = DS2TMNDataset(images_dir=cfg.IMG_ROOT, json_paths=test_jsons, max_instances=cfg.MAX_INSTANCES, category_map=CAT_MAP)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# RAM tüketimini kontrol altında tutmak için otomatik büyütmeyi kaldırdık\n",
    "bs = cfg.BATCH_SIZE\n",
    "# Colab'de worker öldürme sorunlarını önlemek için num_workers=0; pin_memory bellek için kapalı\n",
    "train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=0, pin_memory=cfg.DATA_LOADER_PIN_MEMORY, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=bs, shuffle=False, num_workers=0, pin_memory=cfg.DATA_LOADER_PIN_MEMORY, collate_fn=collate_fn)\n",
    "print({'batch_size': bs, 'train_len': len(train_ds), 'test_len': len(test_ds)})\n",
    "\n",
    "# Etiket aralığı hızlı kontrol (örneklem)\n",
    "def _check_label_ranges(ds, name, max_samples=100):\n",
    "    seen_min, seen_max, checked, bad = None, None, 0, 0\n",
    "    for i in range(min(max_samples, len(ds))):\n",
    "        _, t = ds[i]\n",
    "        l = t['labels']\n",
    "        if l.numel() == 0:\n",
    "            continue\n",
    "        lmin = int(l.min())\n",
    "        lmax = int(l.max())\n",
    "        seen_min = lmin if seen_min is None else min(seen_min, lmin)\n",
    "        seen_max = lmax if seen_max is None else max(seen_max, lmax)\n",
    "        if (l < 1).any() or (l > (NUM_CLASSES-1)).any():\n",
    "            bad += 1\n",
    "        checked += 1\n",
    "    print({name: {'sample_min': seen_min, 'sample_max': seen_max, 'checked': checked, 'out_of_range_samples': bad}})\n",
    "\n",
    "_check_label_ranges(train_ds, 'train_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d317b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modülerleştirme ve Model Kurulumu\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "num_classes = NUM_CLASSES  # background(0) + K kategori (1..K)\n",
    "model = maskrcnn_resnet50_fpn(weights=None)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print('Model on', device, '| num_classes =', num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd6b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitim Döngüsü (AMP) ve Checkpoint\n",
    "from torch.optim import SGD\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os, time, gc\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=cfg.LR, momentum=cfg.MOMENTUM, weight_decay=cfg.WEIGHT_DECAY)\n",
    "scaler = GradScaler('cuda', enabled=torch.cuda.is_available())\n",
    "CKPT_DIR = os.path.join(RUN_DIR, 'checkpoints')\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(cfg.EPOCHS):\n",
    "    t0 = time.time(); total = 0.0\n",
    "    for images, targets in train_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) if torch.is_tensor(v) else v for k,v in t.items()} for t in targets]\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast('cuda', enabled=torch.cuda.is_available()):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "        scaler.scale(losses).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total += losses.item()\n",
    "        # Batch sonu temizlik (özellikle RAM/VRAM baskısını azaltmak için)\n",
    "        del images, targets, loss_dict, losses\n",
    "    dur = time.time()-t0\n",
    "    avg = total / max(1,len(train_loader))\n",
    "    logging.info({'epoch': epoch+1, 'loss': round(avg,3), 'sec': round(dur,1)})\n",
    "    ckpt = os.path.join(CKPT_DIR, f'maskrcnn_epoch{epoch+1}.pt')\n",
    "    torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch+1}, ckpt)\n",
    "    logging.info({'saved': ckpt})\n",
    "    # Epoch sonu bellek temizliği\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Değerlendirme ve Görselleştirme\n",
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "@torch.no_grad()\n",
    "def eval_show(n=3, thr=0.5):\n",
    "    shown = 0\n",
    "    for images, targets in test_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "        for img, out in zip(images, outputs):\n",
    "            if shown>=n: return\n",
    "            fig, ax = plt.subplots(figsize=(6,6))\n",
    "            ax.imshow(img.permute(1,2,0).cpu().numpy())\n",
    "            boxes = out['boxes'].cpu().numpy(); scores = out['scores'].cpu().numpy()\n",
    "            for b,s in zip(boxes, scores):\n",
    "                if s<thr: continue\n",
    "                x1,y1,x2,y2 = b\n",
    "                ax.add_patch(plt.Rectangle((x1,y1), x2-x1, y2-y1, fill=False, color='y', linewidth=2))\n",
    "            ax.set_title(f'detections >= {thr}')\n",
    "            plt.show(); plt.close(fig)  # Bellek sızıntılarını önlemek için figürü kapat\n",
    "            shown += 1\n",
    "\n",
    "eval_show(3, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO mAP Evaluation (train/test sabit eşleme, RAM dostu akış)\n",
    "import json, os, gc\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# Split seçimi: 'train' -> train_jsons & train_loader, 'test' -> test_jsons & test_loader\n",
    "split = (cfg.EVAL_SPLIT or 'test').lower()\n",
    "if split == 'test':\n",
    "    assert len(test_jsons) > 0, \"test split için JSON bulunamadı (ör. deepscores_test.json)\"\n",
    "    EVAL_JSON = test_jsons[0]\n",
    "    eval_loader = test_loader\n",
    "elif split == 'train':\n",
    "    assert len(train_jsons) > 0, \"train split için JSON bulunamadı (ör. deepscores_train.json)\"\n",
    "    EVAL_JSON = train_jsons[0]\n",
    "    eval_loader = train_loader\n",
    "else:\n",
    "    raise ValueError(f\"Bilinmeyen EVAL_SPLIT: {split}. 'train' veya 'test' olmalı.\")\n",
    "\n",
    "print({'eval_split': split, 'json': EVAL_JSON})\n",
    "cocoGt = COCO(EVAL_JSON)\n",
    "\n",
    "# Akış tabanlı tespit toplama: JSONL dosyasına yaz, bellekte tutma\n",
    "REPORT_DIR = os.path.join(RUN_DIR, 'reports')\n",
    "os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "DETS_JSONL = os.path.join(REPORT_DIR, f'detections_{split}.jsonl')\n",
    "DETS_JSON = os.path.join(REPORT_DIR, f'detections_{split}.json')  # COCO loadRes JSON array ister\n",
    "\n",
    "model.eval()\n",
    "import numpy as np\n",
    "@torch.no_grad()\n",
    "def collect_detections_stream(score_thr=0.05):\n",
    "    # Var olan dosyayı temizle\n",
    "    if os.path.exists(DETS_JSONL):\n",
    "        os.remove(DETS_JSONL)\n",
    "    processed = 0\n",
    "    with open(DETS_JSONL, 'w', encoding='utf-8') as fw:\n",
    "        for images, targets in eval_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            for out, tgt in zip(outputs, targets):\n",
    "                img_id = int(tgt['image_id'].item())\n",
    "                boxes = out['boxes'].detach().cpu().numpy()\n",
    "                scores = out['scores'].detach().cpu().numpy()\n",
    "                labels = out['labels'].detach().cpu().numpy()\n",
    "                for b, s, lab in zip(boxes, scores, labels):\n",
    "                    if s < score_thr:\n",
    "                        continue\n",
    "                    x1,y1,x2,y2 = b\n",
    "                    # Eğitim label'ını COCO GT'deki orijinal category_id'ye geri çevir\n",
    "                    orig_cat = int(INV_CAT_MAP.get(int(lab), int(lab)))\n",
    "                    rec = {\n",
    "                        'image_id': img_id,\n",
    "                        'category_id': orig_cat,\n",
    "                        'bbox': [float(x1), float(y1), float(x2-x1), float(y2-y1)],\n",
    "                        'score': float(s)\n",
    "                    }\n",
    "                    fw.write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
    "            # Bellek temizliği (batch bazlı)\n",
    "            del images, outputs\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            processed += len(targets)\n",
    "    return processed\n",
    "\n",
    "# Tüm seçilen split'i işle (RAM şişmeden)\n",
    "processed = collect_detections_stream(score_thr=cfg.EVAL_SCORE_THR)\n",
    "print(f'Processed images ({split}):', processed)\n",
    "\n",
    "# JSONL -> JSON Array (stream ederek)\n",
    "with open(DETS_JSONL, 'r', encoding='utf-8') as fr, open(DETS_JSON, 'w', encoding='utf-8') as fw:\n",
    "    fw.write('[')\n",
    "    first = True\n",
    "    for line in fr:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if not first:\n",
    "            fw.write(',')\n",
    "        fw.write(line)\n",
    "        first = False\n",
    "    fw.write(']')\n",
    "\n",
    "# Evaluate mAP\n",
    "# COCO loadRes dosya yolunu kabul eder (JSON array formatında)\n",
    "try:\n",
    "    cocoDt = cocoGt.loadRes(DETS_JSON)\n",
    "except Exception as e:\n",
    "    print('Failed to load detections for COCOeval:', e)\n",
    "    cocoDt = None\n",
    "\n",
    "if cocoDt is None:\n",
    "    print('No detections to evaluate or load failed.')\n",
    "else:\n",
    "    cocoEval = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()\n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        'AP@[.5:.95]': float(cocoEval.stats[0]),\n",
    "        'AP@0.5': float(cocoEval.stats[1]),\n",
    "        'AP@0.75': float(cocoEval.stats[2])\n",
    "    }\n",
    "    with open(os.path.join(REPORT_DIR, f'metrics_{split}.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics, f)\n",
    "    print('Saved metrics to', os.path.join(REPORT_DIR, f'metrics_{split}.json'))\n",
    "    # Bellek temizliği\n",
    "    del cocoDt, cocoEval\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Çıktı Özeti (RUN_DIR, checkpoint ve raporlar)\n",
    "import os, glob, json\n",
    "\n",
    "if 'RUN_DIR' not in globals():\n",
    "    print('RUN_DIR tanımlı değil. Önce Config hücresini çalıştırın.')\n",
    "else:\n",
    "    print('RUN_DIR          :', RUN_DIR)\n",
    "    ckpt_dir = os.path.join(RUN_DIR, 'checkpoints')\n",
    "    rep_dir  = os.path.join(RUN_DIR, 'reports')\n",
    "\n",
    "    print('\\n[checkpoints]')\n",
    "    ckpts = sorted(glob.glob(os.path.join(ckpt_dir, '*.pt')))\n",
    "    print('adet:', len(ckpts))\n",
    "    if ckpts:\n",
    "        print('son:', ckpts[-1])\n",
    "\n",
    "    print('\\n[reports]')\n",
    "    if os.path.isdir(rep_dir):\n",
    "        reps = sorted(glob.glob(os.path.join(rep_dir, '*')))\n",
    "        for p in reps[:20]:\n",
    "            print('-', os.path.basename(p))\n",
    "        # metrics_<split>.json varsa göster\n",
    "        for split_name in ['train', 'test']:\n",
    "            mpath = os.path.join(rep_dir, f'metrics_{split_name}.json')\n",
    "            if os.path.exists(mpath):\n",
    "                try:\n",
    "                    with open(mpath, 'r', encoding='utf-8') as f:\n",
    "                        metrics = json.load(f)\n",
    "                    print(f\"\\nmetrics_{split_name}.json:\", metrics)\n",
    "                except Exception as e:\n",
    "                    print(f\"metrics_{split_name}.json okunamadı:\", e)\n",
    "    else:\n",
    "        print('Rapor klasörü yok: önce değerlendirme hücresini çalıştırın.')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
